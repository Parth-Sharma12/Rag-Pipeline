{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a2c632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###DATA LOADING MODULE###\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5989a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='This is the content of the document' metadata={'author': 'Parth Sharma', 'created_on': '2025-09-21', 'page': 1}\n"
     ]
    }
   ],
   "source": [
    "doc = Document(\n",
    "    page_content = \"This is the content of the document\",\n",
    "    metadata ={\n",
    "        \"author\": \"Parth Sharma\",\n",
    "        \"created_on\": \"2025-09-21\",\n",
    "        \"page\": 1\n",
    "    }\n",
    ")\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02fee581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': '../data/textfiles/llm.txt', 'author': 'Parth Sharma'}\n"
     ]
    }
   ],
   "source": [
    "## File/document loader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"../data/textfiles/llm.txt\")\n",
    "document = loader.load()\n",
    "meta = document[0].metadata\n",
    "meta.update({\"author\":\"Parth Sharma\"})\n",
    "document[0].metadata = meta\n",
    "print(document[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd6e2bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '..\\\\data\\\\textfiles\\\\llm.txt'}, page_content='LLM means large language model. An LLM is an artificial intelligence system designed to understand and generate human language, typically trained on vast amounts of text data.'), Document(metadata={'source': '..\\\\data\\\\textfiles\\\\rag.txt'}, page_content='RAG stands for Retrieval-Augmented Generation. It is an AI technique that combines information retrieval with language generation, allowing models to fetch relevant documents and generate responses based on both retrieved data and their own knowledge.')]\n"
     ]
    }
   ],
   "source": [
    "## Directory loader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "loader = DirectoryLoader(\"../data/textfiles\",glob =\"*.txt\",loader_cls=TextLoader)\n",
    "documents = loader.load()\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c950bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Desktop\\RAG-Data_ingestion\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': 'Harnessing the Universal Geometry of Embeddings', 'author': 'Rishi Jha; Collin Zhang; Vitaly Shmatikov; John X. Morris', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='arXiv:2505.12540v3  [cs.LG]  25 Jun 2025\\nHarnessing the Universal Geometry of Embeddings\\nRishi Jha\\nCollin Zhang\\nVitaly Shmatikov\\nJohn X. Morris\\nDepartment of Computer Science\\nCornell University\\nAbstract\\nWe introduce the first method for translating text embeddings from one vector\\nspace to another without any paired data, encoders, or predefined sets of matches.\\nOur unsupervised approach translates any embedding to and from a universal latent\\nrepresentation (i.e., a universal semantic structure conjectured by the Platonic\\nRepresentation Hypothesis). Our translations achieve high cosine similarity across\\nmodel pairs with different architectures, parameter counts, and training datasets.\\nThe ability to translate unknown embeddings into a different space while preserving\\ntheir geometry has serious implications for security. An adversary with access\\nto a database of only embedding vectors can extract sensitive information about\\nunderlying documents, sufficient for classification and attribute inference.\\nFigure 1: Left: input embeddings from different model families (T5-based GTR [46] and BERT-based\\nGTE [32]) are fundamentally incomparable. Right: given unpaired embedding samples from different\\nmodels on different texts, our model learns a latent representation where they are closely aligned.\\n1\\nIntroduction\\nText embeddings are the backbone of modern NLP, powering tasks like retrieval, RAG, classification,\\nand clustering. There are many embedding models trained on different datasets, data shufflings, and\\ninitializations. An embedding of a text encodes its semantics: a good model maps texts with similar\\nsemantics to vectors close to each other in the embedding space. Since semantics is a property of\\ntext, different embeddings of the same text should encode the same semantics. In practice, however,\\ndifferent models encode texts into completely different and incompatible vector spaces.\\nThe Platonic Representation Hypothesis [18] conjectures that all image models of sufficient size\\nconverge to the same latent representation. We propose a stronger, constructive version of this\\nhypothesis for text models: the universal latent structure of text representations can be learned and,\\nfurthermore, harnessed to translate representations from one space to another without any paired data\\nor encoders.\\nPreprint. Under review.'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': 'Harnessing the Universal Geometry of Embeddings', 'author': 'Rishi Jha; Collin Zhang; Vitaly Shmatikov; John X. Morris', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='Wool \\nGlobal wool production \\nis about 2 million \\ntonnes per year, of \\nwhich 60% goes into \\napparel. Wool \\ncomprises ca \\nEncoder A\\nDocuments (D)\\nSubject: Enron \\nBashing on Frontline\\nBody:\\nKaren, please call \\nme when you \\nreceive this email. \\nThank you. Rick\\nDocuments (W)\\nDocuments (D)\\n(Recovered)\\nEmbeddings of W\\n(Encoder B)\\nEncoder B\\nEmbeddings of D\\n(Encoder B)\\nSecret\\nPublic\\nSome emails \\ndiscussing NROn \\nEmployee/s \\nComplaint To \\nthePublic (RS-ES \\nstatement of \\nlate/commissary \\nspeaker ﬁrm (...)\\nA1\\nA2\\nT\\nB2\\nB1\\nEmbeddings of D\\n(Encoder A)\\nFigure 2: Given only a vector database from an unknown model, vec2vec translates the database\\ninto the space of a known model using latent structure alone. Converted embeddings reveal sensitive\\ninformation about the original documents, such as the topic of an email (pictured, real example).\\nIn this work, we show that the Strong Platonic Representation Hypothesis holds in practice. Given\\nunpaired examples of embeddings from two models with different architectures and training data, our\\nmethod learns a latent representation in which the embeddings are almost identical (Figure 1).\\nWe draw inspiration from research on aligning word embeddings across languages [61, 10, 15, 9]\\nand unsupervised image translation [35, 68]. Our vec2vec method uses adversarial losses and cycle\\nconsistency to learn to encode embeddings into a shared latent space and decode with minimal loss.\\nThis makes unsupervised translation possible. We use a basic adversarial approach with vector space\\npreservation [45] to learn a mapping from an unknown embedding distribution to a known one.\\nvec2vec is the first method to successfully translate embeddings from the space of one model to\\nanother without paired data.1 vec2vec translations achieve cosine similarity as high as 0.92 to the\\nground-truth vectors in their target embedding spaces and perfect matching on over 8000 shuffled\\nembeddings (without access to the set of possible matches in advance).\\nTo show that our translations preserve not only the relative geometry of embeddings but also the\\nsemantics of underlying inputs, we extract information from them using zero-shot attribute inference\\nand inversion, without any knowledge of the model that produced the original embeddings.2\\n2\\nProblem formulation: unsupervised embedding translation\\nConsider a collection of embedding vectors {u1, . . . un}, for example, a dump of a compromised\\nvector database, where each ui = M1(di) is generated by an unknown encoder M1 : Vs →RdM1\\nfrom an unknown document di. We cannot make queries to M1 and do not know its training data,\\nnor architectural details. Our goal is to extract any information about the documents di.\\nWe do assume access to a different encoder M2 that we can query at will to generate new embeddings\\nin some other space. We also assume high-level distributional knowledge about the hidden docu-\\nments: their modality (text) and language (e.g., English). To extract information, we may translate\\n{u1, . . . un} into the output space of M2 and apply techniques like inversion that take advantage of\\nthe encoder.\\nLimitations of correspondence methods. There is significant prior research on the problem of\\nmatching or correspondence between sets of embedding vectors [1, 48, 8, 53]. These methods\\ntypically assume that the two (or more) sets of embeddings are generated by different encoders on\\nthe same or highly-overlapping inputs. In other words, for each unknown vector, there must already\\nexist a set of candidate vectors in a different embedding. In practice, it is unrealistic to expect that\\nsuch a database be available, so these methods are not directly applicable. Some matching methods,\\n1Prior work has successfully translated word embeddings between languages, typically relying on overlapping\\nvocabularies across languages. In contrast, we translate embeddings of entire sequences between model spaces.\\n2Our code is available on GitHub.\\n2'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': 'Harnessing the Universal Geometry of Embeddings', 'author': 'Rishi Jha; Collin Zhang; Vitaly Shmatikov; John X. Morris', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='Subject: Enron Bashing \\non Frontline\\nBody:\\nKaren, please call me \\nwhen you receive this \\nemail. Thank you. Rick\\nDocument di\\nvi\\nui\\nF(ui)\\nvec2vec\\nVisible to vec2vec\\nInvisible to vec2vec\\nF(ui) ≈ vi\\nUnknown Embedding Space\\nKnown Embedding Space\\nM1\\nFigure 3: Unsupervised embedding translation. With access to only ui = M1(di), vec2vec\\nseeks to generate a translation F(ui) that is close in M2’s embedding space to the ideal embedding\\nvi = M2(di) without access to di, vi, or M1.\\nhowever, support translation between embedding spaces without overlapping inputs. Our experiments\\ndemonstrate that these methods struggle significantly, even when correspondence exists.\\nOur task is inherently more challenging than matching, because we do not assume access to encoder\\nM1, nor do we have additional representations of documents d1, . . . , dn beyond their embeddings\\nui = M1(di). Therefore, we rely solely on unsupervised translation from M1 to M2. The effective-\\nness of such unsupervised translation approaches thus critically depends on identifying and leveraging\\nshared geometric structures within the embedding spaces produced by M1 and M2.\\nThe Strong Platonic Representation Hypothesis. Our hope that unsupervised embedding translation\\nis possible at all rests on the stronger version of the Platonic Representation Hypothesis [18]. Our\\nconjecture is as follows: neural networks trained with the same objective and modality, but with\\ndifferent data and model architectures, converge to a universal latent space such that a translation\\nbetween their respective representations can learned without any pairwise correspondence.\\nTranslation enables information extraction. Solving unsupervised translation will allow us to\\nuse information extraction tools designed to operate on vectors produced by known encoders. For\\nexample, we could apply inversion models [42, 66] to recover unknown documents {di}.\\n3\\nOur method: vec2vec\\nUnsupervised translation has been successful in computer vision, using a combination of cycle\\nconsistency and adversarial regularization [35, 68]. Our design of vec2vec is inspired in part by\\nthese methods. We aim to learn embedding-space translations that are cycle-consistent (mapping to\\nand from an embedding space should end in the same place) and indistinguishable (embeddings for\\nthe same text from either space should have identical latents).\\n3.1\\nArchitecture\\nWe propose a modular architecture, where embeddings are encoded and decoded using space-specific\\nadapter modules and passed through a shared backbone network. Figure 2 shows these components.\\nInput adapters A1 : Rd →RZ and A2 : Rd →RZ transform embeddings from each encoder-specific\\nspace into a universal latent representation of dimension Z. The shared backbone T : RZ →RZ\\nextracts a common latent embedding from adapted inputs. Output adapters B1 : RZ →Rd and\\nB2 : RZ →Rd translate these common latent embeddings back into the encoder-specific spaces.\\nThus, translation functions F1, F2 and additional reconstruction mappings R1, R2 are defined as:\\nF1 = B2 ◦T ◦A1,\\nF2 = B1 ◦T ◦A2\\nR1 = B1 ◦T ◦A1\\nR2 = B2 ◦T ◦A2\\nParameters of all components are collectively denoted θ = {A1, A2, T, B1, B2}.\\nUnlike images, embeddings do not have any spatial bias. Instead of CNNs, we use multilayer percep-\\ntrons (MLP) with residual connections, layer normalization, and SiLU nonlinearities. Discriminators\\nmirror this structure but omit residual connections to simplify adversarial learning.\\n3'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': 'Harnessing the Universal Geometry of Embeddings', 'author': 'Rishi Jha; Collin Zhang; Vitaly Shmatikov; John X. Morris', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='3.2\\nOptimization\\nIn addition to the ‘generator’ networks F and R, we introduce discriminators operating on both the\\nlatent representations of F (Dℓ\\n1, Dℓ\\n2) and the output embeddings (D1, D2).\\nOur goal is to train the parameters of θ by solving:\\nθ∗= arg min\\nθ\\nmax\\nD1,D2,Dℓ\\n1,Dℓ\\n2\\nLadv(F1, F2, D1, D2, Dℓ\\n1, Dℓ\\n2) + λgenLgen(θ),\\n(1)\\nwhere Ladv and Lgen represent adversarial and generator-specific constraints respectively and hyper-\\nparameter λgen controls their tradeoff.\\nAdversarial loss. The adversarial loss encourages generated embeddings to match the empirical\\ndistributions of original embeddings both at the embedding and latent levels. Specifically, applying\\nthe standard GAN loss formulation [13] to both levels yields:\\nLadv(F1, F2, D1, D2, Dℓ\\n1, Dℓ\\n2) = LGAN(D1, F1) + LGAN(D2, F2)\\n+ LGAN(Dℓ\\n1, T ◦A1) + LGAN(Dℓ\\n2, T ◦A2).\\nGenerator. Because adversarial losses alone do not guarantee that translated embeddings preserve\\nsemantics [68], we introduce three additional constraints to help the generator learn a useful mapping:\\nReconstruction enforces that an embedding, when mapped into the latent space and back into its\\noriginal embedding space, closely matches its initial representation:\\nLrec(R1, R2) = Ex∼p∥R1(x) −x∥2\\n2 + Ey∼q∥R2(y) −y∥2\\n2.\\nwhere p and q are distributions of embeddings sampled from M1 and M2, respectively.\\nCycle-consistency acts as an unsupervised proxy for supervised pair alignment, ensuring that F and\\nG can translate an embedding to the other embedding space and back again with minimal corruption:\\nLCC(F1, F2) = Ex∼p∥F2(F1(x)) −x∥2\\n2 + Ey∼q∥F1(F2(y)) −y∥2\\n2.\\nVector space preservation (VSP) ensures that pairwise relationships between generated embeddings\\nremain consistent under translation [45, 64]. Given a batch of B embeddings x1, ..., xB and y1, ..., yB,\\nwe sum their average pairwise distances after translation by both F1 and F2:\\nLVSP(F1, F2) = 1\\nB\\nB\\nX\\ni=1\\nB\\nX\\nj=1\\n\\x14\\n∥M1(xi) · M1(xj) −F1(M1(xi)) · F1(M1(xj))∥2\\n2\\n+ ∥M2(yi) · M2(yj) −F2(M2(yi)) · F2(M2(yj))∥2\\n2\\n\\x15\\nCombining these losses yields: Lgen(θ) = λrecLrec(R1, R2) + λCCLCC(F1, F2) + λVSPLVSP(F1, F2),\\nwhere hyperparameters λCC, λrec, and λVSP control relative importance.\\n4\\nExperimental setup\\n4.1\\nPreliminaries\\nDatasets. We use the Natural Questions (NQ) [25] dataset of user queries and Wikipedia-sourced\\nanswers for training (a 2-million subset) and evaluation (a 65536 subset). To evaluate information\\nextraction, we use TweetTopic [2], a dataset of tweets multi-labeled by 19 topics; a random 8192-\\nrecord subset of Pseudo Re-identified MIMIC-III (MIMIC) [28], a pseudo re-identified version of the\\nMIMIC dataset [19] of patient records multi-labeled by 2673 MedCAT [24] disease descriptions; and\\na random 50-email subset of the Enron Email Corpus (Enron) [21], an unlabeled, public dataset of\\ninternal emails of a defunct energy company.\\nModels. Table 1 lists the embedding models representing three size categories, four transformer\\nbackbones, and two output dimensionalities. Granite is multilingual, CLIP is multimodal.\\n4'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': 'Harnessing the Universal Geometry of Embeddings', 'author': 'Rishi Jha; Collin Zhang; Vitaly Shmatikov; John X. Morris', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='Model\\nParams (M)\\nBackbone\\nYear\\nDims\\nMax Seq.\\n[46] gtr\\n110\\nT5\\n2021\\n768\\n512\\n[49] clip\\n151\\nCLIP\\n2021\\n512\\n77\\n[57] e5\\n109\\nBERT\\n2022\\n768\\n512\\n[32] gte\\n109\\nBERT\\n2023\\n768\\n512\\n[67] stella\\n109\\nBERT\\n2023\\n768\\n512\\n[14] granite\\n278\\nRoBERTa\\n2024\\n768\\n512\\nTable 1: Embedding models used in our experiments.\\nTraining. Unless otherwise specified, each vec2vec is trained on two sets of embeddings generated\\nfrom disjoint sets of 1 million 64-token sequences sampled from NQ (see Appendix D for experiments\\nwith fewer embeddings). Due to GAN instability [52], we select the best of multiple initializations\\nand leave more robust training to future work.\\n4.2\\nEvaluating translation\\nLet ui = M1(di) and vi = M2(di) denote the source and target embeddings of the same input di.\\nThe goal of translation is to generate a vector that is as close to vi as possible. We say that (ui, vj) are\\n“aligned” by the translator F if vj is the closest embedding to F(ui): j = arg mink cos\\n\\x00F(ui), vk\\n\\x01\\n.\\nA perfect translator F ∗satisfies i = arg mink cos\\n\\x00F ∗(ui), vk\\n\\x01\\nfor all i.\\nGiven (unknown) embeddings {M2(dj)}n\\nj=0 ordered by decreasing cosine similarity to F(ui), let ri\\nbe the rank of the correct embedding vi = M2(di). To measure quality of F, we use three metrics.\\nMean Cosine Similarity measures how close translations are, on average, to their targets. Top-1\\nAccuracy is the fraction of translations whose target is closer than any other embedding. Mean\\nRank is the average rank of targets with respect to translations. The ideal translator F ∗achieves\\nmean similarity of 1.0, top-1 accuracy of 1.0, and mean rank of 1.0. Recall that a random alignment\\ncorresponds to a mean rank of n\\n2 . Formally,\\ncos(ui, vi) = 1\\nn\\nn\\nX\\ni=1\\n\\x02\\n1 −cos\\n\\x00F(ui), vi\\n\\x01\\x03\\nTop-1(r) = 1\\nn\\nn\\nX\\ni=1\\n1{ri = 1}\\nRank(r) = 1\\nn\\nn\\nX\\ni=1\\nri\\nvec2vec is the first unsupervised embedding translator, thus there is no direct baseline. As our Naïve\\nbaseline, we simply use F(x) = x to measure geometric similarity between embedding spaces. The\\nsecond (pseudo)baseline is Oracle-aided optimal transport. It assumes that candidate targets are\\nknown and is thus strictly easier than vec2vec and the Naïve baseline. We solve optimal assignment,\\nπ∗= arg minπ\\nPn\\ni=1 cos(ui, vπ(i)), via either the Hungarian, Earth Mover’s Distance, Sinkhorn, or\\nGromov-Wasserstein algorithms, choosing the solver with the lowest rank for each experiment.\\n4.3\\nEvaluating information extraction\\nWe measure whether translation preserves semantics via attribute inference: for each translated\\nembedding F(M1(di)), our goal is to infer attributes ci ⊆C of di.\\nThe first method we use is zero-shot embedding attribute inference: calculate pairwise cosine\\nsimilarities between F(M1(di)) and the embeddings of all attributes in C, identify top k closest\\nattributes, and measure whether they are correct via top-k accuracy: 1\\nn\\nPn\\ni=0 1\\n\\x08\\n|ck\\ni ∩ci| ≥1\\n\\t\\n.\\nThe second method is embedding inversion that recovers text inputs from embeddings. Since [42]\\nrequires a pre-trained inversion model for each embedding space, we use [66] instead to generate an\\napproximation d′\\ni of di from F(M1(di)) in a zero-shot manner. We measure the extracted information\\nusing LLM judge accuracy: the fraction of translated embeddings for which GPT-4o determines that\\nd′ reveals information in d. See Appendix E for our prompt.\\nIn addition to the Naïve baseline, we also consider an Oracle attribute inference: zero-shot\\nclassification with the correct embedding M2(d) and class labels M2(C).\\n5'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': 'Harnessing the Universal Geometry of Embeddings', 'author': 'Rishi Jha; Collin Zhang; Vitaly Shmatikov; John X. Morris', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='e5\\ngranite\\ngte\\ngtr\\nstella\\nM2\\ne5\\ngranite\\ngte\\ngtr\\nstella\\nM1\\n0.02\\n0.68\\n0.03\\n0.38\\n0.02\\n0.01\\n-0.02\\n0.01\\n0.68\\n0.01\\n0.03\\n0.56\\n0.03\\n-0.02\\n0.03\\n0.00\\n0.38\\n0.01\\n0.56\\n0.00\\nSimilarity of Inputs\\ne5\\ngranite\\ngte\\ngtr\\nstella\\nM2\\ne5\\ngranite\\ngte\\ngtr\\nstella\\n0.88\\n0.85\\n0.75\\n0.82\\n0.88\\n0.90\\n0.75\\n0.92\\n0.85\\n0.90\\n0.91\\n0.96\\n0.75\\n0.75\\n0.91\\n0.90\\n0.82\\n0.92\\n0.96\\n0.90\\nSimilarity of Latents\\ne5\\ngranite\\ngte\\ngtr\\nstella\\nM2\\ne5\\ngranite\\ngte\\ngtr\\nstella\\n0.86\\n0.17\\n0.72\\n0.45\\n0.86\\n0.89\\n0.77\\n0.91\\n0.17\\n0.89\\n0.87\\n0.40\\n0.72\\n0.77\\n0.87\\n0.90\\n0.45\\n0.91\\n0.40\\n0.90\\nDifference in Similarities\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\nFigure 4: Pairwise cosine similarities of input embeddings (left) and their vec2vec latents (middle)\\nacross different embedding pairs. The absolute difference between the heatmaps plots is on the right.\\nAll numbers are computed on the same batch of 1024 NQ texts.\\n5\\nvec2vec learns to translate embeddings without any paired data\\nWe first show that vec2vec learns a universal latent space, then demonstrate that this space preserves\\nthe geometry of all embeddings. Therefore, we can use it like a universal language of text encoders\\nto translate their representations without any paired data.\\nvec2vec learns a universal latent space. vec2vec projects embeddings M1,2,... into a shared\\nlatent space via compositions of input adapters (A1,2,...) and a shared translator T. Figure 4 shows\\nthat even when the embeddings ui = M1(di) and vi = M2(di) are far apart (i.e., have low cosine\\nsimilarity), their representations in vec2vec’s latent space are incredibly close: T(A1(ui)) ≈\\nT(A2(vi)). Figure 1 visualizes this (via two-dimensional projections) for vec2vec trained on gte\\nand gtr embeddings: the embeddings are far apart, but their latents are nearly overlapping.\\nvec2vec\\nNaïve Baseline\\nOT Baseline\\nM1\\nM2\\ncos(·) ↑\\nT-1 ↑\\nRank ↓\\ncos(·) ↑\\nT-1 ↑\\nRank ↓\\ncos(·) ↑\\nT-1 ↑\\nRank ↓\\ngra.\\ngtr\\n0.80 (0.0)\\n0.99\\n1.19 (0.1)\\n-0.03 (0.0)\\n0.00\\n4168.73 (9.2)\\n0.50 (0.0)\\n0.00\\n4094.22 (9.2)‡\\ngte\\n0.87 (0.0)\\n0.95\\n1.18 (0.0)\\n0.01 (0.0)\\n0.00\\n4088.58 (9.2)\\n0.85 (0.0)\\n0.00\\n4069.91 (9.2)†\\nste.\\n0.79 (0.0)\\n0.98\\n1.05 (0.0)\\n0.01 (0.0)\\n0.00\\n4208.26 (9.2)\\n0.45 (0.0)\\n0.00\\n4096.35 (9.2)‡\\ne5\\n0.85 (0.0)\\n0.98\\n1.11 (0.0)\\n0.02 (0.0)\\n0.00\\n4111.60 (9.2)\\n0.68 (0.0)\\n0.00\\n4096.17 (9.2)‡\\ngtr\\ngra.\\n0.81 (0.0)\\n0.99\\n1.02 (0.0)\\n-0.03 (0.0)\\n0.00\\n4169.76 (9.2)\\n0.50 (0.0)\\n0.00\\n4093.55 (9.2)‡\\ngte\\n0.87 (0.0)\\n0.93\\n2.31 (0.1)\\n0.04 (0.0)\\n0.00\\n4080.92 (9.2)\\n0.85 (0.0)\\n0.00\\n4079.92 (9.2)†\\nste.\\n0.80 (0.0)\\n0.99\\n1.03 (0.0)\\n0.00 (0.0)\\n0.00\\n4198.78 (9.2)\\n0.46 (0.0)\\n0.00\\n4093.85 (9.2)‡\\ne5\\n0.83 (0.0)\\n0.84\\n2.88 (0.2)\\n0.03 (0.0)\\n0.00\\n4082.84 (9.2)\\n0.83 (0.0)\\n0.00\\n4066.42 (9.2)†\\ngte\\ngra.\\n0.75 (0.0)\\n0.95\\n1.22 (0.0)\\n0.01 (0.0)\\n0.00\\n4079.81 (9.3)\\n0.69 (0.0)\\n0.00\\n4069.23 (9.2)†\\ngtr\\n0.75 (0.0)\\n0.91\\n2.64 (0.1)\\n0.04 (0.0)\\n0.00\\n4084.15 (9.2)\\n0.70 (0.0)\\n0.00\\n4078.45 (9.2)†\\nste.\\n0.89 (0.0)\\n1.00\\n1.00 (0.0)\\n0.56 (0.0)\\n1.00\\n1.00 (0.0)\\n0.69 (0.0)\\n1.00\\n1.00 (0.0)†\\ne5\\n0.87 (0.0)\\n0.99\\n5.19 (0.5)\\n0.68 (0.0)\\n1.00\\n1.00 (0.0)\\n0.83 (0.0)\\n1.00\\n1.00 (0.0)†\\nste.\\ngra.\\n0.80 (0.0)\\n0.98\\n1.08 (0.0)\\n0.01 (0.0)\\n0.00\\n4209.08 (9.3)\\n0.48 (0.0)\\n0.00\\n4096.38 (9.2)‡\\ngtr\\n0.82 (0.0)\\n1.00\\n1.10 (0.0)\\n0.00 (0.0)\\n0.00\\n4192.31 (9.2)\\n0.50 (0.0)\\n0.00\\n4095.46 (9.2)‡\\ngte\\n0.92 (0.0)\\n1.00\\n1.00 (0.0)\\n0.56 (0.0)\\n1.00\\n1.00 (0.0)\\n0.86 (0.0)\\n1.00\\n1.00 (0.0)†\\ne5\\n0.86 (0.0)\\n1.00\\n1.00 (0.0)\\n0.38 (0.0)\\n0.99\\n1.03 (0.0)\\n0.83 (0.0)\\n1.00\\n1.00 (0.0)†\\ne5\\ngra.\\n0.81 (0.0)\\n0.99\\n2.20 (0.2)\\n0.02 (0.0)\\n0.00\\n4120.60 (9.3)\\n0.69 (0.0)\\n0.00\\n4096.12 (9.2)‡\\ngtr\\n0.74 (0.0)\\n0.82\\n2.56 (0.0)\\n0.03 (0.0)\\n0.00\\n4080.76 (9.3)\\n0.70 (0.0)\\n0.00\\n4065.74 (9.2)†\\ngte\\n0.90 (0.0)\\n1.00\\n1.01 (0.0)\\n0.68 (0.0)\\n1.00\\n1.00 (0.0)\\n0.86 (0.0)\\n1.00\\n1.00 (0.0)†\\nste.\\n0.78 (0.0)\\n1.00\\n1.00 (0.0)\\n0.38 (0.0)\\n1.00\\n1.00 (0.0)\\n0.68 (0.0)\\n1.00\\n1.00 (0.0)†\\nTable 2: In-distribution translations: vec2vecs trained on NQ and evaluated on a 65536 text subset of\\nNQ (chunked in batches of size 8192). The rank metric varies from 1 to 8192, thus 4096 corresponds\\nto a random ordering. Standard errors are shown in parentheses. Bold denotes best value. Symbols\\ndenote the lowest-rank solver for specific experiments: Sinkhorn† and Gromov-Wasserstein‡.\\n6'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': 'Harnessing the Universal Geometry of Embeddings', 'author': 'Rishi Jha; Collin Zhang; Vitaly Shmatikov; John X. Morris', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='TweetTopic\\nMIMIC\\nM1\\nM2\\ncos(·) ↑\\nTop-1 ↑\\nRank ↓\\ncos(·) ↑\\nTop-1 ↑\\nRank ↓\\ngran.\\ngtr\\n0.74 (0.0)\\n0.99\\n1.09 (0.1)\\n0.74 (0.0)\\n0.60\\n23.38 (1.6)\\ngte\\n0.85 (0.0)\\n0.95\\n1.26 (0.1)\\n0.85 (0.0)\\n0.08\\n346.21 (7.8)\\nstel.\\n0.77 (0.0)\\n0.96\\n1.11 (0.0)\\n0.72 (0.0)\\n0.13\\n242.23 (6.1)\\ne5\\n0.83 (0.0)\\n0.87\\n3.10 (0.7)\\n0.84 (0.0)\\n0.12\\n361.06 (8.7)\\ngtr\\ngran.\\n0.79 (0.0)\\n0.98\\n2.41 (0.6)\\n0.78 (0.0)\\n0.51\\n35.27 (1.9)\\ngte\\n0.85 (0.0)\\n0.96\\n1.29 (0.2)\\n0.84 (0.0)\\n0.12\\n279.56 (6.9)\\nstel.\\n0.77 (0.0)\\n0.96\\n1.10 (0.0)\\n0.72 (0.0)\\n0.27\\n127.92 (4.4)\\ne5\\n0.80 (0.0)\\n0.53\\n13.38 (1.2)\\n0.82 (0.0)\\n0.01\\n1413.80 (18.3)\\ngte\\ngran.\\n0.73 (0.0)\\n0.94\\n1.33 (0.1)\\n0.73 (0.0)\\n0.09\\n342.15 (7.8)\\ngtr\\n0.71 (0.0)\\n0.95\\n1.29 (0.1)\\n0.69 (0.0)\\n0.12\\n256.63 (6.4)\\nstel.\\n0.86 (0.0)\\n1.00\\n1.00 (0.0)\\n0.85 (0.0)\\n1.00\\n1.00 (0.0)\\ne5\\n0.83 (0.0)\\n0.91\\n1.57 (0.2)\\n0.86 (0.0)\\n0.54\\n17.71 (0.9)\\nstel.\\ngran.\\n0.79 (0.0)\\n0.99\\n1.09 (0.1)\\n0.77 (0.0)\\n0.14\\n221.95 (5.9)\\ngtr\\n0.77 (0.0)\\n1.00\\n1.00 (0.0)\\n0.75 (0.0)\\n0.56\\n17.70 (1.0)\\ngte\\n0.90 (0.0)\\n1.00\\n1.00 (0.0)\\n0.91 (0.0)\\n1.00\\n1.00 (0.0)\\ne5\\n0.85 (0.0)\\n0.98\\n1.05 (0.0)\\n0.85 (0.0)\\n0.51\\n26.33 (1.2)\\ne5\\ngran.\\n0.79 (0.0)\\n0.98\\n1.08 (0.0)\\n0.78 (0.0)\\n0.21\\n151.09 (4.6)\\ngtr\\n0.67 (0.0)\\n0.80\\n3.10 (0.6)\\n0.66 (0.0)\\n0.01\\n1029.64 (14.9)\\ngte\\n0.87 (0.0)\\n0.99\\n1.02 (0.0)\\n0.87 (0.0)\\n0.60\\n32.59 (2.6)\\nstel.\\n0.75 (0.0)\\n0.98\\n1.06 (0.0)\\n0.75 (0.0)\\n0.46\\n32.12 (1.4)\\nTable 3: Out-of-distribution translations: vec2vecs trained on NQ and evaluated on the entire\\nTweetTopic test set (800 tweets) and an 8192-record subset of MIMIC. The rank metric varies from 1\\nto 800 (for TweetTopic) and 8192 (for MIMIC), thus 400 and, respectively, 4096 correspond to a\\nrandom ordering. Standard errors are shown in parentheses.\\nvec2vec translations mirror target geometry. Table 2 shows that vec2vec generates embeddings\\nwith near-optimal assignment across model pairs, achieving cosine similarity scores up to 0.92, top-1\\naccuracies up to 100%, and ranks as low as 1. In same-backbone pairings (e.g., (gte, e5)), vec2vec’s\\ntop-1 accuracy and rank are comparable to both the naïve baseline and (surprisingly) the oracle-aided\\noptimal transport. Although the embeddings generated by vec2vec are significantly closer to the\\nground truth than the naïve baseline, in same-backbone pairings the embeddings are close enough to\\nbe compatible. In cross-backbone pairings, vec2vec is far superior on all metrics, while baseline\\nmethods perform similarly to random guessing.\\nTable 3 shows that this performance extends to out-of-distribution data. Our vec2vec translators\\nwere trained on NQ (drawn from Wikipedia), yet exhibit high cosine similarity, high accuracy, and\\nlow rank when evaluated on tweets (which are far more colloquial and use emojis) and medical\\nrecords (which contain domain-specific jargon unlikely to appear in NQ). In Appendix C, we show\\nthat baseline methods fail on cross-backbone embedding pairs.\\nvec2vec\\nOT Baseline\\nM1\\nM2\\ncos(·) ↑\\nT-1 ↑\\nRank ↓\\ncos(·) ↑\\nT-1 ↑\\nRank ↓\\ngra.\\nclip\\n0.78 (0.0)\\n0.35\\n226.62 (3.2)\\n0.60 (0.0)\\n0.00\\n4096.00 (9.2)\\ngtr\\n0.73 (0.0)\\n0.13\\n711.23 (5.9)\\n0.59 (0.0)\\n0.00\\n4096.78 (9.2)\\ngte\\n0.62 (0.0)\\n0.00\\n3233.41 (9.8)\\n0.59 (0.0)\\n0.00\\n4096.16 (9.2)\\nste.\\n0.77 (0.0)\\n0.31\\n286.69 (3.6)\\n0.59 (0.0)\\n0.00\\n4096.76 (9.2)\\ne5\\n0.64 (0.0)\\n0.01\\n2568.21 (9.4)\\n0.60 (0.0)\\n0.00\\n4096.41 (9.2)\\nclip\\ngra.\\n0.74 (0.0)\\n0.72\\n4.46 (0.1)\\n0.48 (0.0)\\n0.00\\n4096.86 (9.2)\\ngtr\\n0.67 (0.0)\\n0.27\\n155.11 (2.1)\\n0.49 (0.0)\\n0.00\\n4096.35 (9.2)\\ngte\\n0.75 (0.0)\\n0.00\\n2678.90 (8.9)\\n0.72 (0.0)\\n0.00\\n4096.44 (9.2)\\nste.\\n0.72 (0.0)\\n0.61\\n22.50 (0.5)\\n0.45 (0.0)\\n0.00\\n4096.27 (9.2)\\ne5\\n0.73 (0.0)\\n0.01\\n1692.28 (8.2)\\n0.68 (0.0)\\n0.00\\n4096.42 (9.2)\\nTable 4: Translations between unimodal and multimodal (CLIP) embeddings: vec2vecs trained on\\nNQ and evaluated on a 65536 text subset of NQ (chunked in batches of size 8192). Rank varies from\\n1 to 8192, thus 4096 corresponds to a random ordering. Since the embedding dimensionalities are\\ndifferent, only the Gromov-Wasserstein OT baseline is run and the naive baseline does not apply.\\nBold denotes best value.\\n7'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': 'Harnessing the Universal Geometry of Embeddings', 'author': 'Rishi Jha; Collin Zhang; Vitaly Shmatikov; John X. Morris', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='TweetTopic (k = 1)\\nMIMIC (k = 10)\\nM1\\nM2\\nvec2vec\\nNaïve\\nM1\\nM2\\nvec2vec\\nNaïve\\nM1\\nM2\\ngran.\\ngtr\\n0.25\\n0.10\\n0.30\\n0.24\\n0.19\\n0.11\\n0.76\\n0.88\\ngte\\n0.32\\n0.09\\n0.30\\n0.34\\n0.36\\n0.13\\n0.76\\n1.00\\nstel.\\n0.24\\n0.10\\n0.30\\n0.28\\n0.27\\n0.04\\n0.76\\n0.96\\ne5\\n0.31\\n0.18\\n0.30\\n0.31\\n0.19\\n0.20\\n0.76\\n0.97\\ngtr\\ngran.\\n0.34\\n0.08\\n0.24\\n0.30\\n0.16\\n0.12\\n0.88\\n0.76\\ngte\\n0.33\\n0.13\\n0.24\\n0.34\\n0.28\\n0.05\\n0.88\\n1.00\\nstel.\\n0.30\\n0.10\\n0.24\\n0.28\\n0.25\\n0.07\\n0.88\\n0.96\\ne5\\n0.30\\n0.04\\n0.24\\n0.31\\n0.09\\n0.09\\n0.88\\n0.97\\ngte\\ngran.\\n0.37\\n0.04\\n0.34\\n0.30\\n0.18\\n0.11\\n1.00\\n0.76\\ngtr\\n0.24\\n0.13\\n0.34\\n0.24\\n0.10\\n0.03\\n1.00\\n0.88\\nstel.\\n0.31\\n0.20\\n0.34\\n0.28\\n0.68\\n0.83\\n1.00\\n0.96\\ne5\\n0.37\\n0.30\\n0.34\\n0.31\\n0.37\\n0.63\\n1.00\\n0.97\\nstel.\\ngran.\\n0.35\\n0.07\\n0.28\\n0.30\\n0.23\\n0.09\\n0.96\\n0.76\\ngtr\\n0.26\\n0.13\\n0.28\\n0.24\\n0.22\\n0.09\\n0.96\\n0.88\\ngte\\n0.38\\n0.36\\n0.28\\n0.34\\n0.90\\n0.98\\n0.96\\n1.00\\ne5\\n0.35\\n0.34\\n0.28\\n0.31\\n0.38\\n0.46\\n0.96\\n0.97\\ne5\\ngran.\\n0.33\\n0.15\\n0.31\\n0.30\\n0.14\\n0.07\\n0.97\\n0.76\\ngtr\\n0.26\\n0.22\\n0.31\\n0.24\\n0.11\\n0.04\\n0.97\\n0.88\\ngte\\n0.34\\n0.28\\n0.31\\n0.34\\n0.47\\n0.66\\n0.97\\n1.00\\nstel.\\n0.26\\n0.16\\n0.31\\n0.28\\n0.36\\n0.40\\n0.97\\n0.96\\nTable 5: Information leakage via top-k zero-shot attribute inference: vec2vecs trained on NQ and\\nevaluated on the TweetTopic test set (800 tweets) and an 8192-record subset of MIMIC. M1 and M2\\nrepresent ideal zero-shot inference: attributes and embeddings are encoded using the same model.\\nFinally, Table 4 shows that vec2vec can even translate to and from the space of CLIP, a multimodal\\nembedding model which was trained in part on image data. While the translations are not as strong as\\nin Table 2, vec2vec consistently outperforms the optimal transport baseline. These results show the\\npromise of our method at adapting to new modalities: in particular, the embedding space of CLIP has\\nbeen successfully connected to other modalities such as heatmaps, audio, and depth charts [12].\\n6\\nUsing vec2vec translations to extract information\\nIn this section, we show that vec2vec translations not only preserve the geometric structure of\\nembeddings but also retain sufficient semantics to enable attribute inference.\\ne5\\ngranite gte\\ngtr\\nstella\\nM2\\ne5\\ngranite\\ngte\\ngtr\\nstella\\nM1\\n49\\n52\\n28\\n62\\n55\\n56\\n56\\n56\\n58\\n40\\n34\\n80\\n20\\n60\\n46\\n55\\n56\\n52\\n66\\n40\\nPercentage of Emails With Leakage\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\nFigure 5: Leakage of information via in-\\nversion. Trained on NQ and evaluated on\\na 50-email subset of the Enron Email Cor-\\npus. Cells denote judge accuracy.\\nZero-shot attribute inference.\\nTable 5 shows that\\nattribute inference on vec2vec translations consistently\\noutperforms the naïve baseline and often does better than\\nthe ideal zero-shot baseline which performs inference\\non ground-truth document and attribute embeddings in\\nthe same space (this baseline is imaginary since these\\nembeddings are not available in our setting).\\nvec2vec translations even work for embeddings of med-\\nical records, which are much further from the training\\ndistribution than tweets. The attributes in this case are\\nMedCAT disease descriptions, very few of which occur\\nin the training data. Attribute inference on translated\\nembeddings is comparable to the naïve baseline in same-\\nbackbone pairings and outperforms it (often greatly) in\\ncross-backbone pairings. The fact that vec2vec pre-\\nserves the semantics of concepts like \"alveolar periosti-\\ntis\" (which never appears in its training data) is evidence\\nthat its latent space is indeed a universal representation.\\nZero-shot inversion. Inversion, i.e., reconstruction of\\ntext inputs, is more ambitious than attribute inference. vec2vec translations retain enough semantic\\ninformation that off-the-shelf, zero-shot inversion methods like [66], developed for embeddings\\n8'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': 'Harnessing the Universal Geometry of Embeddings', 'author': 'Rishi Jha; Collin Zhang; Vitaly Shmatikov; John X. Morris', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='computed by standard encoders, extract information for as many as 80% of documents given only\\ntheir translated embeddings, for some model pairs (Figure 5). These inversions are imperfect and we\\nleave development of specialized inverters for translated embeddings to future work. Nevertheless, as\\nexemplified in Figure 6, they still extract information such as individual and company names, dates,\\npromotions, financial information, outages, and even lunch orders. In Appendix E, we show the\\nprompt we use to measure extraction.\\nGround Truth: “Subject: Enron\\nBashing on Frontline \\\\n Body:...\"\\nGeneration: “Some emails discussing NROn Employee/s Complaint To thePublic ...\"\\nGround Truth: “Subject: Trades for 3/1/02 \\\\n Body: \\\\n John , \\\\n The following trades...\"\\nGeneration: “... future transactions may await John G...\"\\nGround Truth: “ The following expense report is ready for approval...\"\\nGeneration: “ The upcoming expense statement from YYYY MM Dec...\"\\nFigure 6: Examples of inversions that infer entities and content .\\n7\\nRelated work\\nRepresentation alignment. Similarities between representations of different neural networks are\\ninvestigated in [26, 31, 58, 5, 18, 60, 30]. Methods based on CCA [41], SVCCA, [50], CKA [23, 37],\\nICA [62], time-series [38], and GUIs [16] have been used to compare embeddings from different\\nsubspaces. [36, 44, 39, 56, 47] harness representation similarity for zero-shot stitching, substitution,\\ndomain transfer, and multi-modal adaptation. All rely on some amount of paired data, which is\\ndifficult to reduce [6]. Our method does not just measure similarity, we learn how to translate\\nrepresentations across spaces without any paired data.\\nOptimal transport. The problem of unsupervised optimal transport has been studied for image style\\ntransfer [17, 35, 68], word translation [61, 10, 15, 9, 20], and natural language sequence translation\\n[51, 27, 1, 4, 63, 3]. Our method builds on these works, which often employ a combination of\\ncycle-consistency and adversarial loss. Importantly, unlike prior word and sequence translation\\nmethods, multiple representations of the same input (e.g., heavily overlapping word vocabularies) are\\nunavailable in our setting. [53] proposes a solver for matching small sets of embeddings between\\ndifferent vision-language models. Our method goes well beyond matching by taking unknown\\nembeddings and generating matching embeddings in the space of another model.\\nEmbedding inversion. An emerging line of research investigates decoding text from language model\\nembeddings [54, 29, 42] and outputs [43, 7, 65]. vec2vec helps apply these to unknown embeddings,\\nwithout an encoder or paired data, by translating them to the space of a known model.\\nBridging modality gaps. Previous work has noted an inherent “gap” between image- and text-based\\nmodels [33] and proposed various ways to unify the modalities [55]. Some approaches feed image\\nembeddings directly into language models [22, 59, 11, 34], while others generate captions from\\nimage embeddings [40] or even from text embeddings themselves [42]. [12] introduces a shared\\nembedding space that integrates inputs from multiple modalities, including text, audio, and vision. In\\ncontrast, our post-hoc approach directly translates between representations and complements these\\nsystems by enabling inputs from a wide variety of embedding models.\\n8\\nDiscussion and Future Work\\nThe Platonic Representation Hypothesis conjectures that the representation spaces of modern neural\\nnetworks are converging. We assert the Strong Platonic Representation Hypothesis: the latent\\nuniversal representation can be learned and harnessed to translate between representation spaces\\nwithout any encoders or paired data.\\n9'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': 'Harnessing the Universal Geometry of Embeddings', 'author': 'Rishi Jha; Collin Zhang; Vitaly Shmatikov; John X. Morris', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='In Section 5, we demonstrated that our vec2vec method successfully translates embeddings generated\\nfrom unseen documents by unseen encoders, and the translator is robust to (sometimes very) out-\\nof-distribution inputs. This suggests that vec2vec learns domain-agnostic translations based on the\\nuniversal geometric relationships which encode the same semantics in multiple embedding spaces.\\nIn Section 6, we showed that vec2vec translations preserve sufficient input semantics to enable\\nattribute inference. We extracted sensitive disease information from patient records and partial content\\nfrom corporate emails, with access only to document embeddings and no access to the encoder that\\nproduced them. Better translation methods will enable higher-fidelity extraction, confirming once\\nagain that embeddings reveal (almost) as much as their inputs.\\nOur findings provide compelling evidence for the Strong Platonic Representation Hypothesis for text-\\nbased models. Our preliminary results on CLIP suggest that the universal geometry can be harnessed\\nin other modalities, too. The results in this paper are but a lower bound on inter-representation\\ntranslation. Better and more stable learning algorithms, architectures, and other methodological\\nimprovements will support scaling to more data, more model families, and more modalities.\\nAcknowledgments and Disclosure of Funding\\nThis research is supported in part by the Google Cyber NYC Institutional Research Program. JM is\\nsupported by the National Science Foundation.\\nReferences\\n[1]\\nDavid Alvarez-Melis and Tommi S. Jaakkola. “Gromov-Wasserstein Alignment of Word\\nEmbedding Spaces”. 2018. arXiv: 1809.00013 [cs.CL].\\n[2]\\nDimosthenis Antypas, Asahi Ushio, Francesco Barbieri, and Jose Camacho-Collados. “Mul-\\ntilingual Topic Classification in X: Dataset and Analysis”. In: Proceedings of the 2024 Con-\\nference on Empirical Methods in Natural Language Processing. Ed. by Yaser Al-Onaizan,\\nMohit Bansal, and Yun-Nung Chen. Miami, Florida, USA: Association for Computational\\nLinguistics, Nov. 2024, pp. 20136–20152.\\n[3]\\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. “A robust self-learning method for fully\\nunsupervised cross-lingual mappings of word embeddings”. In: Proceedings of the 56th Annual\\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers). Ed. by\\nIryna Gurevych and Yusuke Miyao. Melbourne, Australia: Association for Computational\\nLinguistics, July 2018, pp. 789–798.\\n[4]\\nMikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. “Unsupervised Neural\\nMachine Translation”. 2018. arXiv: 1710.11041 [cs.CL].\\n[5]\\nYamini Bansal, Preetum Nakkiran, and Boaz Barak. “Revisiting Model Stitching to Compare\\nNeural Representations”. 2021. arXiv: 2106.07682 [cs.LG].\\n[6]\\nIrene Cannistraci, Luca Moschella, Valentino Maiorca, Marco Fumero, Antonio Norelli, and\\nEmanuele Rodolà. “Bootstrapping Parallel Anchors for Relative Representations”. 2023. arXiv:\\n2303.00721 [cs.LG].\\n[7]\\nNicholas Carlini, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke, Jonathan\\nHayase, A. Feder Cooper, Katherine Lee, Matthew Jagielski, Milad Nasr, Arthur Conmy,\\nItay Yona, Eric Wallace, David Rolnick, and Florian Tramèr. “Stealing Part of a Production\\nLanguage Model”. 2024. arXiv: 2403.06634 [cs.CR].\\n[8]\\nLiqun Chen, Zhe Gan, Yu Cheng, Linjie Li, Lawrence Carin, and Jingjing Liu. “Graph Optimal\\nTransport for Cross-Domain Alignment”. 2020. arXiv: 2006.14744 [cs.CL].\\n[9]\\nXilun Chen and Claire Cardie. “Unsupervised Multilingual Word Embeddings”. 2018. arXiv:\\n1808.08933 [cs.CL].\\n[10]\\nAlexis Conneau, Guillaume Lample, Marc’Aurelio Ranzato, Ludovic Denoyer, and Hervé\\nJégou. “Word Translation Without Parallel Data”. 2018. arXiv: 1710.04087 [cs.CL].\\n[11]\\nRunpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao,\\nJianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma,\\nand Li Yi. “DreamLLM: Synergistic Multimodal Comprehension and Creation”. 2024. arXiv:\\n2309.11499 [cs.CV].\\n10'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': 'Harnessing the Universal Geometry of Embeddings', 'author': 'Rishi Jha; Collin Zhang; Vitaly Shmatikov; John X. Morris', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='[12]\\nRohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala,\\nArmand Joulin, and Ishan Misra. “ImageBind: One Embedding Space To Bind Them All”.\\n2023. arXiv: 2305.05665 [cs.CV].\\n[13]\\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\\nOzair, Aaron Courville, and Yoshua Bengio. “Generative adversarial networks”. In: Commun.\\nACM 63.11 (Oct. 2020), pp. 139–144.\\n[14]\\nIBM Granite Embedding Team. “Granite Embedding Models”. Dec. 2024.\\n[15]\\nEdouard Grave, Armand Joulin, and Quentin Berthet. “Unsupervised Alignment of Embed-\\ndings with Wasserstein Procrustes”. 2018. arXiv: 1805.11222 [cs.LG].\\n[16]\\nFlorian Heimerl, Christoph Kralj, Torsten Moller, and Michael Gleicher. “embComp: Visual\\nInteractive Comparison of Vector Embeddings”. In: IEEE Transactions on Visualization and\\nComputer Graphics 28.8 (Aug. 2022), pp. 2953–2969.\\n[17]\\nXun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. “Multimodal Unsupervised Image-\\nto-Image Translation”. 2018. arXiv: 1804.04732 [cs.CV].\\n[18]\\nMinyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola. “The Platonic Representa-\\ntion Hypothesis”. 2024. arXiv: 2405.07987 [cs.LG].\\n[19]\\nAlistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad\\nGhassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. “MIMIC-\\nIII, a freely accessible critical care database”. In: Scientific data 3.1 (2016), pp. 1–9.\\n[20]\\nArmand Joulin, Piotr Bojanowski, Tomas Mikolov, Hervé Jégou, and Edouard Grave. “Loss in\\nTranslation: Learning Bilingual Word Mapping with a Retrieval Criterion”. In: Proceedings of\\nthe 2018 Conference on Empirical Methods in Natural Language Processing. Ed. by Ellen\\nRiloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii. Brussels, Belgium: Association\\nfor Computational Linguistics, Oct. 2018, pp. 2979–2984.\\n[21]\\nBryan Klimt and Yiming Yang. “The enron corpus: a new dataset for email classification\\nresearch”. In: Proceedings of the 15th European Conference on Machine Learning. ECML’04.\\nPisa, Italy: Springer-Verlag, 2004, pp. 217–226.\\n[22]\\nJing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. “Grounding language models to images\\nfor multimodal inputs and outputs”. In: International Conference on Machine Learning. PMLR.\\n2023, pp. 17283–17300.\\n[23]\\nSimon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. “Similarity of\\nNeural Network Representations Revisited”. 2019. arXiv: 1905.00414 [cs.LG].\\n[24]\\nZeljko Kraljevic, Thomas Searle, Anthony Shek, Lukasz Roguski, Kawsar Noor, Daniel\\nBean, Aurelie Mascio, Leilei Zhu, Amos A Folarin, Angus Roberts, et al. “Multi-domain\\nclinical natural language processing with MedCAT: the medical concept annotation toolkit”.\\nIn: Artificial intelligence in medicine 117 (2021), p. 102083.\\n[25]\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh,\\nChris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova,\\nLlion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le,\\nand Slav Petrov. “Natural Questions: A Benchmark for Question Answering Research”. In:\\nTransactions of the Association for Computational Linguistics 7 (2019). Ed. by Lillian Lee,\\nMark Johnson, Brian Roark, and Ani Nenkova, pp. 452–466.\\n[26]\\nAarre Laakso and Garrison Cottrell. “Content and cluster analysis: Assessing representational\\nsimilarity in neural systems”. In: Philosophical Psychology 13.1 (2000), pp. 47–76.\\n[27]\\nGuillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio Ranzato. “Unsu-\\npervised Machine Translation Using Monolingual Corpora Only”. 2018. arXiv: 1711.00043\\n[cs.CL].\\n[28]\\nEric Lehman, Sarthak Jain, Karl Pichotta, Yoav Goldberg, and Byron Wallace. “Does BERT\\nPretrained on Clinical Notes Reveal Sensitive Data?” In: Proceedings of the 2021 Conference\\nof the North American Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies. Ed. by Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer,\\nDilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and\\nYichao Zhou. Online: Association for Computational Linguistics, June 2021, pp. 946–959.\\n[29]\\nHaoran Li, Mingshi Xu, and Yangqiu Song. “Sentence Embedding Leaks More Information\\nthan You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence”.\\n2023. arXiv: 2305.03010 [cs.CL].\\n11'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': 'Harnessing the Universal Geometry of Embeddings', 'author': 'Rishi Jha; Collin Zhang; Vitaly Shmatikov; John X. Morris', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='[30]\\nJiaang Li, Yova Kementchedjhieva, Constanza Fierro, and Anders Søgaard. “Do Vision and\\nLanguage Models Share Concepts? A Vector Space Alignment Study”. In: Transactions of the\\nAssociation for Computational Linguistics 12 (2024), pp. 1232–1249.\\n[31]\\nYixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John Hopcroft. “Convergent Learning:\\nDo different neural networks learn the same representations?” 2016. arXiv: 1511.07543\\n[cs.LG].\\n[32]\\nZehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang.\\n“Towards General Text Embeddings with Multi-stage Contrastive Learning”. 2023. arXiv:\\n2308.03281 [cs.CL].\\n[33]\\nWeixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Zou. “Mind the Gap:\\nUnderstanding the Modality Gap in Multi-modal Contrastive Representation Learning”. 2022.\\narXiv: 2203.02053 [cs.CL].\\n[34]\\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. “Visual Instruction Tuning”.\\n2023. arXiv: 2304.08485 [cs.CV].\\n[35]\\nMing-Yu Liu, Thomas Breuel, and Jan Kautz. “Unsupervised Image-to-Image Translation\\nNetworks”. 2018. arXiv: 1703.00848 [cs.CV].\\n[36]\\nValentino Maiorca, Luca Moschella, Antonio Norelli, Marco Fumero, Francesco Locatello,\\nand Emanuele Rodolà. “Latent Space Translation via Semantic Alignment”. 2024. arXiv:\\n2311.00664 [cs.LG].\\n[37]\\nMayug Maniparambil, Raiymbek Akshulakov, Yasser Abdelaziz Dahou Djilali, Mohamed\\nEl Amine Seddik, Sanath Narayan, Karttikeya Mangalam, and Noel E O’Connor. “Do Vision\\nand Language Encoders Represent the World Similarly?” In: Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition. 2024, pp. 14334–14343.\\n[38]\\nDeven M Mistry and Ali A Minai. “A Comparative Study of Sentence Embedding Models for\\nAssessing Semantic Variation”. In: International Conference on Artificial Neural Networks.\\n2023, pp. 1–12.\\n[39]\\nMazda Moayeri, Keivan Rezaei, Maziar Sanjabi, and Soheil Feizi. “Text-To-Concept (and\\nBack) via Cross-Model Alignment”. 2023. arXiv: 2305.06386 [cs.CV].\\n[40]\\nRon Mokady, Amir Hertz, and Amit H. Bermano. “ClipCap: CLIP Prefix for Image Caption-\\ning”. 2021. arXiv: 2111.09734 [cs.CV].\\n[41]\\nAri S. Morcos, Maithra Raghu, and Samy Bengio. “Insights on representational similarity in\\nneural networks with canonical correlation”. 2018. arXiv: 1806.05759 [stat.ML].\\n[42]\\nJohn X. Morris, Volodymyr Kuleshov, Vitaly Shmatikov, and Alexander M. Rush. “Text\\nEmbeddings Reveal (Almost) As Much As Text”. 2023. arXiv: 2310.06816 [cs.CL].\\n[43]\\nJohn X. Morris, Wenting Zhao, Justin T. Chiu, Vitaly Shmatikov, and Alexander M. Rush.\\n“Language Model Inversion”. 2023. arXiv: 2311.13647 [cs.CL].\\n[44]\\nLuca Moschella, Valentino Maiorca, Marco Fumero, Antonio Norelli, Francesco Locatello,\\nand Emanuele Rodolà. “Relative representations enable zero-shot latent space communication”.\\n2023. arXiv: 2209.15430 [cs.LG].\\n[45]\\nNikola Mrksic, Diarmuid Ó Séaghdha, Blaise Thomson, Milica Gasic, Lina Rojas-Barahona,\\nPei-Hao Su, David Vandyke, Tsung-Hsien Wen, and Steve Young. “Counter-fitting Word\\nVectors to Linguistic Constraints”. 2016. arXiv: 1603.00892 [cs.CL].\\n[46]\\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández Ábrego, Ji Ma, Vincent Y.\\nZhao, Yi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei Yang. “Large Dual Encoders Are\\nGeneralizable Retrievers”. 2021. arXiv: 2112.07899 [cs.IR].\\n[47]\\nAntonio Norelli, Marco Fumero, Valentino Maiorca, Luca Moschella, Emanuele Rodolà, and\\nFrancesco Locatello. “ASIF: Coupled Data Turns Unimodal Models to Multimodal Without\\nTraining”. 2023. arXiv: 2210.01738 [cs.LG].\\n[48]\\nGabriel Peyré, Marco Cuturi, and Justin Solomon. “Gromov-Wasserstein Averaging of Kernel\\nand Distance Matrices”. In: Proceedings of the 33rd International Conference on Machine\\nLearning (ICML). Vol. 48. JMLR: Workshop and Conference Proceedings. New York, NY,\\nUSA: JMLR, 2016.\\n[49]\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\\nSutskever. “Learning Transferable Visual Models From Natural Language Supervision”. 2021.\\narXiv: 2103.00020 [cs.CV].\\n12'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': 'Harnessing the Universal Geometry of Embeddings', 'author': 'Rishi Jha; Collin Zhang; Vitaly Shmatikov; John X. Morris', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='[50]\\nMaithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. “SVCCA: Singular\\nVector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability”.\\n2017. arXiv: 1706.05806 [stat.ML].\\n[51]\\nSujith Ravi and Kevin Knight. “Deciphering Foreign Language”. In: Proceedings of the\\n49th Annual Meeting of the Association for Computational Linguistics: Human Language\\nTechnologies. Ed. by Dekang Lin, Yuji Matsumoto, and Rada Mihalcea. Portland, Oregon,\\nUSA: Association for Computational Linguistics, June 2011, pp. 12–21.\\n[52]\\nDivya Saxena and Jiannong Cao. “Generative Adversarial Networks (GANs): Challenges,\\nSolutions, and Future Directions”. In: ACM Comput. Surv. 54.3 (May 2021).\\n[53]\\nDominik Schnaus, Nikita Araslanov, and Daniel Cremers. “It’s a (Blind) Match! Towards\\nVision-Language Correspondence without Parallel Data”. 2025. arXiv: 2503.24129 [cs.CV].\\n[54]\\nCongzheng Song and Ananth Raghunathan. “Information Leakage in Embedding Models”.\\n2020. arXiv: 2004.00053 [cs.LG].\\n[55]\\nShezheng Song, Xiaopeng Li, Shasha Li, Shan Zhao, Jie Yu, Jun Ma, Xiaoguang Mao, and\\nWeimin Zhang. “How to Bridge the Gap between Modalities: A Comprehensive Survey on\\nMultimodal Large Language Model”. 2023. arXiv: 2311.07594 [cs.CL].\\n[56]\\nYingtao Tian and Jesse Engel. “Latent translation: Crossing modalities by bridging generative\\nmodels”. In: arXiv preprint arXiv:1902.08261 (2019).\\n[57]\\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan\\nMajumder, and Furu Wei. “Text Embeddings by Weakly-Supervised Contrastive Pre-training”.\\n2024. arXiv: 2212.03533 [cs.CL].\\n[58]\\nLiwei Wang, Lunjia Hu, Jiayuan Gu, Yue Wu, Zhiqiang Hu, Kun He, and John Hopcroft.\\n“Towards Understanding Learning Representations: To What Extent Do Different Neural\\nNetworks Learn the Same Representation”. 2018. arXiv: 1810.11750 [cs.LG].\\n[59]\\nWenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo,\\nTong Lu, Jie Zhou, Yu Qiao, and Jifeng Dai. “VisionLLM: Large Language Model is also an\\nOpen-Ended Decoder for Vision-Centric Tasks”. 2023. arXiv: 2305.11175 [cs.CV].\\n[60]\\nChristopher Wolfram and Aaron Schein. “Layers at similar depths generate similar activations\\nacross llm architectures”. In: arXiv preprint arXiv:2504.08775 (2025).\\n[61]\\nChao Xing, Dong Wang, Chao Liu, and Yiye Lin. “Normalized Word Embedding and Orthog-\\nonal Transform for Bilingual Word Translation”. In: Proceedings of the 2015 Conference of\\nthe North American Chapter of the Association for Computational Linguistics: Human Lan-\\nguage Technologies. Ed. by Rada Mihalcea, Joyce Chai, and Anoop Sarkar. Denver, Colorado:\\nAssociation for Computational Linguistics, May 2015, pp. 1006–1011.\\n[62]\\nHiroaki Yamagiwa, Momose Oyama, and Hidetoshi Shimodaira. “Discovering Universal\\nGeometry in Embeddings with ICA”. 2023. arXiv: 2305.13175 [cs.CL].\\n[63]\\nZhen Yang, Wei Chen, Feng Wang, and Bo Xu. “Unsupervised Neural Machine Translation\\nwith Weight Sharing”. 2018. arXiv: 1804.09057 [cs.CL].\\n[64]\\nJinsung Yoon and Sercan O Arik. “Embedding-Converter: A Unified Framework for Cross-\\nModel Embedding Transformation”. 2025.\\n[65]\\nCollin Zhang, John X. Morris, and Vitaly Shmatikov. “Extracting Prompts by Inverting LLM\\nOutputs”. 2024. arXiv: 2405.15012 [cs.CL].\\n[66]\\nCollin Zhang, John X. Morris, and Vitaly Shmatikov. “Universal Zero-shot Embedding Inver-\\nsion”. 2025. arXiv: 2504.00147 [cs.CL].\\n[67]\\nDun Zhang, Jiacheng Li, Ziyang Zeng, and Fulong Wang. “Jasper and Stella: distillation of\\nSOTA embedding models”. 2025. arXiv: 2412.19048 [cs.IR].\\n[68]\\nJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. “Unpaired Image-to-Image\\nTranslation using Cycle-Consistent Adversarial Networks”. 2020. arXiv: 1703 . 10593\\n[cs.CV].\\n13'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': 'Harnessing the Universal Geometry of Embeddings', 'author': 'Rishi Jha; Collin Zhang; Vitaly Shmatikov; John X. Morris', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='A\\nCompute\\nOur training and evaluation were conducted using diverse compute environments, including both\\nlocal and cloud GPU clusters. Experiments were done on NVIDIA 2080Ti, L4, A40, and A100\\nGPUs, listed in order of increasing computational capacity.\\nFor our main experiments, we trained 36 distinct vec2vec models, with training durations ranging\\nfrom 1 to 7 days per model, depending on the specific GPU and model pair (which affected con-\\nvergence rates). Additional 9 vec2vec models were trained for our ablations, bringing the total to\\n45 vec2vec models. Taking a conservative estimate of the average training time, this amounted to\\napproximately 180 GPU days (45 models × 4 days / model).\\nEvaluation procedures varied by model type:\\n• For each of our 45 vec2vec models, our full evaluation on NQ, TweetTopic, and MIMIC\\ndatasets required approximately 1 hour per model across all GPU types.\\n• The 36 non-ablation GANs underwent additional analysis on the Enron email corpus,\\nrequiring about 1.5 hours for inversion and downstream LLM evaluation.\\n• Naive baselines (30 models without CLIP) required approximately 30 minutes each for\\nevaluation, roughly half the time needed for our full models.\\n• Optimal transport baselines (36 models and 4 different algorithms) were run exclusively on\\nCPU, each requiring approximately 4 hours of computation time.\\nIn total, our experiments consumed approximately 180 GPU days for training and an additional\\n78 GPU hours for evaluation and analysis. There were approximately 144 CPU hours required for\\noptimal transport.\\nB\\nExtended oracle-aided optimal transport baseline results\\nLet ui = M1(di) and vi = M2(di) denote embeddings of the same document di from two different\\nembedding models. In Section 5, we solve the optimal assignment problem:\\nπ∗= arg min\\nπ\\nn\\nX\\ni=1\\ncos(ui, vπ(i)),\\nusing four algorithms: Hungarian (linear sum assignment), Earth Mover’s Distance (EMD), Sinkhorn,\\nand Gromov-Wasserstein. Note that this baseline computes matchings and transports between\\nembeddings derived from the same underlying texts, strongly favoring optimal transport (OT) methods.\\nNevertheless, OT still struggles when embeddings originate from different model backbones.\\nSince the Hungarian algorithm produces a discrete matching, it is evaluated only using Top-1\\nAccuracy, while the other algorithms are evaluated across all metrics. For each experiment, the\\nlowest-rank solver is reported in Table 2 and Table 4 (denoted by symbols in the final column).\\nEvaluation metrics are defined as follows:\\n1. Top-1 Accuracy: Fraction of embeddings correctly identified as closest pairs, calculated by\\neither selecting the maximum transported mass per embedding or applying the Hungarian\\nalgorithm directly to the transport plan P. We report the higher accuracy between the two.\\n2. Mean Rank: Average rank position of the correct embedding match vi when sorted by\\ndescending transported mass Pij from ui:\\nrank(vi) = position of vi among sorted Pij.\\n3. Mean Cosine Similarity: Average cosine similarity between barycenters and true counter-\\nparts:\\nv′\\ni =\\nPn\\nj=1 Pijvj\\nPn\\nj=1 Pij\\n,\\nSimilarity = 1\\nn\\nn\\nX\\ni=1\\ncos(v′\\ni, vi).\\nC\\nFull out-of-distribution translation results\\nWe provide baseline numbers for the experiments shown in Table 3, by dataset.\\n14'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': 'Harnessing the Universal Geometry of Embeddings', 'author': 'Rishi Jha; Collin Zhang; Vitaly Shmatikov; John X. Morris', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14}, page_content='vec2vec\\nNaïve Baseline\\nOT Baseline\\nE1\\nE2\\ncos(·)\\nT-1\\nRank\\ncos(·)\\nT-1\\nRank\\ncos(·)\\nT-1\\nRank\\ngra.\\ngtr\\n0.74 (0.0)\\n0.99\\n1.09 (0.1)\\n-0.04 (0.0)\\n0.00\\n415.61 (8.2)\\n0.51 (0.0)\\n0.01\\n398.32 (8.2)‡\\ngte\\n0.85 (0.0)\\n0.95\\n1.26 (0.1)\\n0.00 (0.0)\\n0.00\\n406.73 (8.2)\\n0.74 (0.0)\\n0.01\\n398.14 (8.2)∗\\nste.\\n0.77 (0.0)\\n0.96\\n1.11 (0.0)\\n0.00 (0.0)\\n0.00\\n417.27 (8.2)\\n0.56 (0.0)\\n0.00\\n399.57 (8.2)‡\\ne5\\n0.83 (0.0)\\n0.87\\n3.10 (0.7)\\n0.02 (0.0)\\n0.00\\n405.53 (8.1)\\n0.75 (0.0)\\n0.01\\n398.33 (8.2)‡\\ngtr\\ngra.\\n0.79 (0.0)\\n0.98\\n2.41 (0.6)\\n-0.04 (0.0)\\n0.00\\n411.53 (8.3)\\n0.57 (0.0)\\n0.01\\n398.29 (8.2)‡\\ngte\\n0.85 (0.0)\\n0.96\\n1.29 (0.2)\\n0.04 (0.0)\\n0.00\\n392.01 (8.2)\\n0.86 (0.0)\\n0.00\\n384.00 (8.1)†\\nste.\\n0.77 (0.0)\\n0.96\\n1.10 (0.0)\\n0.00 (0.0)\\n0.00\\n394.69 (8.3)\\n0.74 (0.0)\\n0.00\\n390.67 (8.2)†\\ne5\\n0.80 (0.0)\\n0.53\\n13.38 (1.2)\\n0.03 (0.0)\\n0.00\\n400.85 (8.2)\\n0.75 (0.0)\\n0.00\\n399.78 (8.2)‡\\ngte\\ngra.\\n0.73 (0.0)\\n0.94\\n1.33 (0.1)\\n0.00 (0.0)\\n0.00\\n408.81 (8.3)\\n0.56 (0.0)\\n0.01\\n398.16 (8.2)∗\\ngtr\\n0.71 (0.0)\\n0.95\\n1.29 (0.1)\\n0.04 (0.0)\\n0.00\\n386.58 (8.3)\\n0.71 (0.0)\\n0.00\\n383.41 (8.1)†\\nste.\\n0.86 (0.0)\\n1.00\\n1.00 (0.0)\\n0.58 (0.0)\\n1.00\\n1.00 (0.0)\\n1.00 (0.0)\\n1.00\\n1.00 (0.0)∗\\ne5\\n0.83 (0.0)\\n0.91\\n1.57 (0.2)\\n0.68 (0.0)\\n1.00\\n1.00 (0.0)\\n1.00 (0.0)\\n1.00\\n1.00 (0.0)∗\\nste.\\ngra.\\n0.79 (0.0)\\n0.99\\n1.09 (0.1)\\n0.00 (0.0)\\n0.00\\n418.16 (8.4)\\n0.57 (0.0)\\n0.00\\n399.56 (8.2)‡\\ngtr\\n0.77 (0.0)\\n1.00\\n1.00 (0.0)\\n0.00 (0.0)\\n0.00\\n393.07 (8.1)\\n0.71 (0.0)\\n0.00\\n390.26 (8.2)†\\ngte\\n0.90 (0.0)\\n1.00\\n1.00 (0.0)\\n0.58 (0.0)\\n1.00\\n1.00 (0.0)\\n1.00 (0.0)\\n1.00\\n1.00 (0.0)∗\\ne5\\n0.85 (0.0)\\n0.98\\n1.05 (0.0)\\n0.37 (0.0)\\n0.89\\n1.55 (0.1)\\n1.00 (0.0)\\n1.00\\n1.00 (0.0)∗\\ne5\\ngra.\\n0.79 (0.0)\\n0.98\\n1.08 (0.0)\\n0.02 (0.0)\\n0.00\\n405.75 (8.3)\\n0.57 (0.0)\\n0.01\\n398.34 (8.2)‡\\ngtr\\n0.67 (0.0)\\n0.80\\n3.10 (0.6)\\n0.03 (0.0)\\n0.00\\n401.16 (8.4)\\n0.51 (0.0)\\n0.00\\n399.73 (8.2)‡\\ngte\\n0.87 (0.0)\\n0.99\\n1.02 (0.0)\\n0.68 (0.0)\\n1.00\\n1.00 (0.0)\\n1.00 (0.0)\\n1.00\\n1.00 (0.0)∗\\nste.\\n0.75 (0.0)\\n0.98\\n1.06 (0.0)\\n0.37 (0.0)\\n1.00\\n1.00 (0.0)\\n1.00 (0.0)\\n1.00\\n1.00 (0.0)∗\\nTable 6: Out-of-distribution translations on TweetTopic (with baselines): vec2vec models trained on\\nNQ and evaluated on the entire TweetTopic test set (800 tweets). The rank metric varies from 1 to\\n800, thus 400 corresponds to a random ordering. Standard errors are shown in parentheses. Symbols\\ndenote the lowest-rank solver: Earth Mover’s Distance∗, Sinkhorn† and Gromov-Wasserstein‡\\nvec2vec\\nNaïve Baseline\\nOT Baseline\\nE1\\nE2\\ncos(·)\\nT-1\\nRank\\ncos(·)\\nT-1\\nRank\\ncos(·)\\nT-1\\nRank\\ngra.\\ngtr\\n0.74 (0.0)\\n0.60\\n23.38 (1.6)\\n-0.02 (0.0)\\n0.00\\n4010.00 (25.8)\\n0.82 (0.0)\\n0.00\\n3962.83 (26.1)†\\ngte\\n0.85 (0.0)\\n0.08\\n346.21 (7.8)\\n0.01 (0.0)\\n0.00\\n3978.35 (26.1)\\n0.92 (0.0)\\n0.00\\n3808.18 (25.9)†\\nste.\\n0.72 (0.0)\\n0.13\\n242.23 (6.1)\\n-0.01 (0.0)\\n0.00\\n3900.74 (26.2)\\n0.86 (0.0)\\n0.02\\n3780.44 (26.0)†\\ne5\\n0.84 (0.0)\\n0.12\\n361.06 (8.7)\\n0.02 (0.0)\\n0.00\\n4024.92 (26.1)\\n0.93 (0.0)\\n0.00\\n3937.63 (26.2)†\\ngtr\\ngra.\\n0.78 (0.0)\\n0.51\\n35.27 (1.9)\\n-0.02 (0.0)\\n0.00\\n4023.67 (26.1)\\n0.87 (0.0)\\n0.00\\n3964.83 (26.1)†\\ngte\\n0.84 (0.0)\\n0.12\\n279.56 (6.9)\\n0.08 (0.0)\\n0.00\\n4180.47 (26.2)\\n0.87 (0.0)\\n0.00\\n4088.97 (26.2)‡\\nste.\\n0.72 (0.0)\\n0.27\\n127.92 (4.4)\\n0.00 (0.0)\\n0.00\\n4296.04 (26.1)\\n0.76 (0.0)\\n0.00\\n4095.11 (26.1)‡\\ne5\\n0.82 (0.0)\\n0.01\\n1413.80 (18.3)\\n0.09 (0.0)\\n0.00\\n4064.47 (26.2)\\n0.93 (0.0)\\n0.00\\n4010.13 (26.1)†\\ngte\\ngra.\\n0.73 (0.0)\\n0.09\\n342.15 (7.8)\\n0.01 (0.0)\\n0.00\\n3946.19 (25.8)\\n0.87 (0.0)\\n0.00\\n3802.92 (25.9)†\\ngtr\\n0.69 (0.0)\\n0.12\\n256.63 (6.4)\\n0.08 (0.0)\\n0.00\\n4229.90 (26.2)\\n0.69 (0.0)\\n0.00\\n4094.02 (26.1)‡\\nste.\\n0.85 (0.0)\\n1.00\\n1.00 (0.0)\\n0.56 (0.0)\\n1.00\\n1.00 (0.0)\\n1.00 (0.0)\\n1.00\\n1.00 (0.0)∗\\ne5\\n0.86 (0.0)\\n0.54\\n17.71 (0.9)\\n0.69 (0.0)\\n0.98\\n1.04 (0.0)\\n1.00 (0.0)\\n1.00\\n1.00 (0.0)∗\\nste.\\ngra.\\n0.77 (0.0)\\n0.14\\n221.95 (5.9)\\n-0.01 (0.0)\\n0.00\\n3951.42 (25.9)\\n0.87 (0.0)\\n0.01\\n3776.52 (26.0)†\\ngtr\\n0.75 (0.0)\\n0.56\\n17.70 (1.0)\\n0.00 (0.0)\\n0.00\\n4339.83 (26.2)\\n0.70 (0.0)\\n0.00\\n4093.61 (26.1)‡\\ngte\\n0.91 (0.0)\\n1.00\\n1.00 (0.0)\\n0.56 (0.0)\\n1.00\\n1.00 (0.0)\\n1.00 (0.0)\\n1.00\\n1.00 (0.0)∗\\ne5\\n0.85 (0.0)\\n0.51\\n26.33 (1.2)\\n0.35 (0.0)\\n0.59\\n12.68 (0.6)\\n0.93 (0.0)\\n1.00\\n1.00 (0.0)†\\ne5\\ngra.\\n0.78 (0.0)\\n0.21\\n151.09 (4.6)\\n0.02 (0.0)\\n0.00\\n4008.10 (25.9)\\n0.87 (0.0)\\n0.00\\n3932.58 (26.2)†\\ngtr\\n0.66 (0.0)\\n0.01\\n1029.64 (14.9)\\n0.09 (0.0)\\n0.00\\n4032.85 (26.2)\\n0.82 (0.0)\\n0.00\\n4010.06 (26.1)†\\ngte\\n0.87 (0.0)\\n0.60\\n32.59 (2.6)\\n0.69 (0.0)\\n0.98\\n1.09 (0.0)\\n1.00 (0.0)\\n1.00\\n1.00 (0.0)∗\\nste.\\n0.75 (0.0)\\n0.46\\n32.12 (1.4)\\n0.35 (0.0)\\n0.86\\n2.49 (0.1)\\n0.86 (0.0)\\n1.00\\n1.01 (0.0)†\\nTable 7: Out-of-distribution translations on MIMIC (with baselines): vec2vec models trained on\\nNQ and evaluated on an 8192-record subset of MIMIC. The rank metric varies from 1 to 8192, thus\\n4096 corresponds to a random ordering. Standard errors are shown in parentheses. Symbols denote\\nthe lowest-rank solver: Earth Mover’s Distance∗, Sinkhorn† and Gromov-Wasserstein‡\\n15'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': 'Harnessing the Universal Geometry of Embeddings', 'author': 'Rishi Jha; Collin Zhang; Vitaly Shmatikov; John X. Morris', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15}, page_content='D\\nAblations\\nD.1\\nAblating components of our method\\nWe ablate our method subtractively, measuring the key metrics after removing individual components\\nof our algorithm (described in Section 3). Table 8 shows that each component appears to be critical to\\nbuilding good translations. While in each setting, vec2vec’s cos(·) is higher than the naïve baselines,\\nthe Top-1 accuracies and ranks imply that ablated vec2vec translations are at best only slightly better\\nthan the baseline and do not preserve the geometry of the vector space.\\nMethod\\ncos(·)\\nTop-1\\nRank\\nvec2vec\\n0.75 (0.0)\\n0.91\\n2.64 (0.1)\\nNaïve Baseline\\n0.04 (0.0)\\n0.00\\n4084.15 (9.2)\\nOT Baseline\\n0.70 (0.0)\\n0.00\\n4078.45 (9.2)\\n– VSP loss\\n0.58 (0.0)\\n0.00\\n4196.64 (9.2)\\n– CC loss\\n0.50 (0.0)\\n0.00\\n3941.36 (9.3)\\n– latent GAN\\n0.49 (0.0)\\n0.00\\n3897.09 (9.5)\\n– VSP and CC loss\\n0.47 (0.0)\\n0.00\\n3365.24 (9.3)\\n– hyperparam. tuning\\n0.50 (0.0)\\n0.00\\n4011.73 (9.3)\\nTable 8: gte →gtr translators trained without individual components of our method on NQ and\\nevaluated on a 65536-text subset of NQ (chunked in batches of 8192). The rank metric varies from 1\\nto 8192, thus 4096 corresponds to a random ordering. Standard errors are shown in parentheses.\\nD.2\\nAmount of data needed to learn translation\\nN\\ncos(·)\\nTop-1\\nRank\\n1000000\\n0.75 (0.0)\\n0.92\\n2.73 (0.2)\\n10000\\n0.57 (0.0)\\n0.01\\n1462.21 (20.)\\n50000\\n0.74 (0.0)\\n0.81\\n3.91 (0.6)\\n100000\\n0.74 (0.0)\\n0.85\\n4.52 (0.4)\\n500000\\n0.75 (0.0)\\n0.92\\n2.73 (0.2)\\nTable 9: gte →gtr translators trained with different amounts of gte data: vec2vec models trained on\\nNQ and evaluated an 8192-record subset of NQ. The rank metric varies from 1 to 8192, thus 4096\\ncorresponds to a random ordering. Standard errors are shown in parentheses.\\nIn our main experiments, we use 1M-point subsets of NQ to train our vec2vec models. Now, we\\ntrain the gtr-gte vec2vec with 1M gtr embeddings but fewer gte embeddings. Table 9 shows that\\ntranslators still learn something (i.e. are better than random), and only 50K embeddings are sufficient\\nto learn a translation that is almost as good. Even with 10K embeddings, our method still learns\\na non-trivial translation. Translation performance generally seems to improve with the amount of\\ntraining data, however.\\nE\\nPrompt for measuring information extraction\\nFigure 5 utilizes an LLM judge to determine whether the approximate inversion (using [66]) of a\\ntranslated email embedding leaks information about the original email. Figure 7 shows our prompt to\\nthe GPT-4o judge:\\n16'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdffiles\\\\embeddings.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': 'Harnessing the Universal Geometry of Embeddings', 'author': 'Rishi Jha; Collin Zhang; Vitaly Shmatikov; John X. Morris', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='Original email: {ground_truth}\\nReconstructed email: {generation}.\\nDoes the reconstructed email leak any information about the original email?\\nAnswer with only ‘yes’ or ‘no’.\\nFigure 7: The prompt given to the LLM judge.\\n17')]\n"
     ]
    }
   ],
   "source": [
    "## Directory loader\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "loader = DirectoryLoader(\"../data/pdffiles\",glob =\"*.pdf\",loader_cls=PyMuPDFLoader)\n",
    "pdf_documents = loader.load()\n",
    "print(pdf_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7135fe78",
   "metadata": {},
   "source": [
    "##### TODO - Excel parsing and DB file parsing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "933a8b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d89730",
   "metadata": {},
   "source": [
    "Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8e766c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Desktop\\RAG-Data_ingestion\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict,Tuple, Any\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86040f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel:\n",
    "    \"\"\" Class to handle text embeddings using SentenceTransformer.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the pre-trained model to use. Defaults to 'all-MiniLM-L6-v2'.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model() ## we write protected method to load the model. this method is not accessible outside the class.\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model.\"\"\"\n",
    "        try:\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Loaded model: {self.model_name}\")\n",
    "            print(f\"model_dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise e\n",
    "\n",
    "    \n",
    "    def parse_documents(self,path: str) -> List[Document]:\n",
    "        \"\"\"Parse documents from the given directory.\n",
    "\n",
    "            Args:\n",
    "                path (str): Path to the directory containing text files.\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(path):\n",
    "            raise ValueError(f\"Provided path {path} is not a valid directory.\")\n",
    "        \n",
    "        loader = DirectoryLoader(path, glob=\"*.pdf\", loader_cls=PyMuPDFLoader)\n",
    "        documents = loader.load()\n",
    "        return documents\n",
    "    \n",
    "    def batch_embed_texts(self, texts: List[str], batch_size: int = 32) -> np.ndarray:\n",
    "        \"\"\"Embed texts in batches to handle large datasets.\n",
    "\n",
    "            Args:\n",
    "                texts (List[str]): List of texts to be embedded.\n",
    "                batch_size (int): Size of each batch. Defaults to 32.\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded.\")\n",
    "        \n",
    "        embeddings =[]\n",
    "        for i in range(0,len(texts),batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            batch_embeddings = self.model.encode(batch_texts, convert_to_numpy=True)\n",
    "            embeddings.append(batch_embeddings)\n",
    "        return np.vstack(embeddings)\n",
    "    \n",
    "    def chunking(self, path:str, chunk_size: int = 500, overlap: int = 50) -> Tuple[List[Document], np.ndarray]:\n",
    "        \"\"\"Chunk the text into smaller pieces.\n",
    "\n",
    "            Args:\n",
    "                text (str): The text to be chunked.\n",
    "                chunk_size (int): The size of each chunk. Defaults to 500.\n",
    "                overlap (int): The number of overlapping characters between chunks. Defaults to 50.\n",
    "\n",
    "            Returns:\n",
    "                List[str]: List of text chunks.\n",
    "        \"\"\"\n",
    "        All_texts = self.parse_documents(path) ## It will bydefault only split page content and not metadata.\n",
    "        text_splitter = RecursiveCharacterTextSplitter( \n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "        chunks = text_splitter.split_documents(All_texts)\n",
    "        embedding = self.batch_embed_texts([chunk.page_content for chunk in chunks])\n",
    "        return chunks, embedding\n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09dad8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: all-MiniLM-L6-v2\n",
      "model_dimension: 384\n",
      "Number of chunks: 134\n",
      "Embedding shape: (134, 384)\n"
     ]
    }
   ],
   "source": [
    "embedding_model = EmbeddingModel()\n",
    "chunks, embeddings_ = embedding_model.chunking(\"../data/pdfFiles\")\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(f\"Embedding shape: {embeddings_.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117fb3d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22efd3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x2613a819710>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "## initialize the embedding manager\n",
    "\n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad245f0",
   "metadata": {},
   "source": [
    "Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd114398",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdfFiles\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e84da852",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "vectorstore.add_documents(chunks, embeddings_)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb6bbd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG-Data_ingestion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
